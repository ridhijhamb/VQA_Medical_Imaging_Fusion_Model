{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "941096e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joyrb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from datasets import *\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d54ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "attn_reg = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6f4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"flaviagiammarino/path-vqa\", split='train[:5000]')\n",
    "val_dataset = load_dataset(\"flaviagiammarino/path-vqa\", split='validation[:1000]')\n",
    "\n",
    "train_images, train_ques, train_ans = train_dataset['image'], train_dataset['question'], train_dataset['answer']\n",
    "val_images, val_ques, val_ans = val_dataset['image'], val_dataset['question'], val_dataset['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe3b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = train_ans + val_ans\n",
    "train_images = [img.resize((224, 224)).convert('RGB') for img in train_images]\n",
    "val_images = [img.resize((224, 224)).convert('RGB') for img in val_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0a05f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val_images[1]).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d91d2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_words(answers):\n",
    "    \"\"\"\n",
    "    Makes word vocabulary to index bag_of_words\n",
    "    \n",
    "    Args:\n",
    "        answers: list of answers\n",
    "    \n",
    "    Returns:\n",
    "        Dict: word to num map\n",
    "        Dict: num to word map\n",
    "    \"\"\"\n",
    "    word_to_int = dict()\n",
    "    int_to_word = dict()\n",
    "    bag_of_words = list()\n",
    "    \n",
    "    for ans in answers:\n",
    "        tokens = word_tokenize(ans)\n",
    "        for t in tokens:\n",
    "            bag_of_words.append(t)\n",
    "            \n",
    "    counter = Counter(bag_of_words)\n",
    "    sorted_counter = sorted(counter.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    for ind, key in enumerate(sorted_counter):\n",
    "        word_to_int[key[0]] = ind+1\n",
    "        int_to_word[ind+1] = key[0]\n",
    "        \n",
    "    word_to_int['<start>'] = len(word_to_int)+1\n",
    "    int_to_word[len(int_to_word)+1] = '<start>'\n",
    "    \n",
    "    word_to_int['<end>'] = len(word_to_int)+1\n",
    "    int_to_word[len(int_to_word)+1] = '<end>'\n",
    "    \n",
    "    word_to_int['<unk>'] = len(word_to_int)+1\n",
    "    int_to_word[len(int_to_word)+1] = '<unk>'\n",
    "    \n",
    "    word_to_int['<pad>'] = 0\n",
    "    int_to_word[0] = '<pad>'\n",
    "    \n",
    "    return word_to_int, int_to_word\n",
    "\n",
    "word_to_int, int_to_word = map_words(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c597b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_answers(answers, word_to_int):\n",
    "    \"\"\"\n",
    "    This method is to encode the tokens in an answer by the corresponding number\n",
    "    \n",
    "    Args:\n",
    "        answers: the list of answers\n",
    "        word_to_int: the mapping of word to number\n",
    "        \n",
    "    Returns:\n",
    "        List of nums: encoded answers\n",
    "        List: of answer lengths\n",
    "    \"\"\"\n",
    "    tokenized_answers = list()\n",
    "    max_len = 0\n",
    "    for answer in answers:\n",
    "        tokens = word_tokenize(answer)\n",
    "        max_len = max(max_len, len(tokens)+2)\n",
    "        tokenized_answers.append(tokens)\n",
    "    \n",
    "    encoded_ans = list()\n",
    "    ans_len = list()\n",
    "    for ind, tokens in enumerate(tokenized_answers):\n",
    "        enc_c = [word_to_int['<start>']] + [word_to_int.get(word, word_to_int['<unk>']) for word in tokens] + [\n",
    "                        word_to_int['<end>']] + [word_to_int['<pad>']] * (max_len - len(tokens))\n",
    "        encoded_ans.append(enc_c)\n",
    "        ans_len.append(len(tokens)+2)\n",
    "        \n",
    "    encoded_ans = np.array(encoded_ans)\n",
    "    ans_len = np.array(ans_len)\n",
    "        \n",
    "    return encoded_ans, ans_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bac0b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def load_image_features(images, model_name='resnet152'):\n",
    "    # Load the pre-trained model\n",
    "    model = models.resnet152(pretrained=True)\n",
    "    model = torch.nn.Sequential(*(list(model.children())[:-1])) # Remove the last layer\n",
    "    model.eval()\n",
    "\n",
    "    # Image transformations\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    fc7_features = []\n",
    "    image_id_list = []\n",
    "\n",
    "    # Iterate over images in the list\n",
    "    for i, image in enumerate(images):\n",
    "        input_tensor = preprocess(image)\n",
    "        input_batch = input_tensor.unsqueeze(0) # Create a mini-batch as expected by the model\n",
    "\n",
    "        # If you have a GPU, put everything on cuda\n",
    "        if torch.cuda.is_available():\n",
    "            input_batch = input_batch.to('cuda')\n",
    "            model.to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_batch)\n",
    "        \n",
    "        output = output.squeeze(0).permute(1, 2, 0)\n",
    "        \n",
    "        # Append features and image id to lists\n",
    "        fc7_features.append(output.cpu().numpy())\n",
    "        image_id_list.append(f'image_{i}')\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(output.shape) \n",
    "        \n",
    "    # Save features and image id list to h5 files\n",
    "    with h5py.File(f'{model_name}_fc7.h5', 'w') as hf:\n",
    "        hf.create_dataset(\"fc7_features\",  data=fc7_features)\n",
    "    with h5py.File(f'{model_name}_image_id_list.h5', 'w') as hf:\n",
    "        hf.create_dataset(\"image_id_list\",  data=image_id_list)\n",
    "\n",
    "    return fc7_features, image_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58ec07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def load_ques_embed(questions, model_name='bert-base-cased'):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate over questions\n",
    "    for question in questions:\n",
    "        # Tokenize the question\n",
    "        inputs = tokenizer(question, return_tensors='pt')\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get the embeddings of the [CLS] token (first token)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "        # Append to the list of embeddings\n",
    "        embeddings.append(cls_embedding)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01ed4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ques_img_pair(img_id_list, img_feat, ques_id_list, ques_embed):\n",
    "    data = dict()\n",
    "    img_id_list_dict = list_to_dict(img_id_list)\n",
    "    img_feat_data = list()\n",
    "    \n",
    "    for ques_id in ques_id_list:\n",
    "        img_feat_data.append(img_feat[img_id_list_dict[ques_id]])\n",
    "        \n",
    "    img_feat_data = np.array(img_feat_data)\n",
    "    \n",
    "    data['img_feat'] = img_feat_data\n",
    "    data['ques_feat'] = ques_embed\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "153ce00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(images, questions, answers, word_to_int, img_model, ques_model, split):\n",
    "    encoded_ans, ans_len = encode_answers(answers, word_to_int)\n",
    "    img_feat, image_id_list = load_image_features(images, 'resnet152')\n",
    "    ques_embed, ques_id_list = load_ques_embed(questions, 'bert-base-cased'), image_id_list\n",
    "    \n",
    "    data = make_ques_img_pair(image_id_list, img_feat, ques_id_list, ques_embed)\n",
    "    data['answer'] = encoded_ans\n",
    "    data['ans_len'] = ans_len\n",
    "    \n",
    "    print(data['img_feat'].shape)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31740c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_dict(id_list):\n",
    "    id_dict = {}\n",
    "    count = 0\n",
    "    for obj_id in id_list:\n",
    "        id_dict[obj_id] = count\n",
    "        count += 1\n",
    "    \n",
    "    return id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "78703053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "(5000, 1, 1, 2048)\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "torch.Size([1, 1, 2048])\n",
      "(1000, 1, 1, 2048)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data(train_images, train_ques, train_ans, word_to_int, 'resnet152', 'bert', 'train')\n",
    "val_data = load_data(val_images, val_ques, val_ans, word_to_int, 'resnet152', 'bert-base-cased', 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "85ee3f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img_feat', 'ques_feat', 'answer', 'ans_len'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2fc576ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2048])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.Tensor(train_data['img_feat'][0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5e0f08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 38)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "print(train_data['answer'].shape)\n",
    "    \n",
    "train_data_tensor = TensorDataset(torch.Tensor(train_data['img_feat']),\n",
    "                                  torch.Tensor(train_data['ques_feat']),\n",
    "                                  torch.Tensor(train_data['answer']),\n",
    "                                  torch.Tensor(train_data['ans_len']))\n",
    "val_data_tensor = TensorDataset(torch.Tensor(val_data['img_feat']),\n",
    "                                  torch.Tensor(val_data['ques_feat']),\n",
    "                                  torch.Tensor(val_data['answer']),\n",
    "                                  torch.Tensor(val_data['ans_len']))\n",
    "\n",
    "train_loader = DataLoader(train_data_tensor, batch_size = 32, shuffle=True)\n",
    "val_loader = DataLoader(val_data_tensor, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f3e8a2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "for img, q, a, a_len in train_loader:\n",
    "    print(q.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7e53e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size, LSTM_units, LSTM_layers, feat_size,\n",
    "                 batch_size, global_avg_pool_size, \n",
    "                 dropout = 0.3, mfb_output_dim = 5000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.mfb_output_dim = mfb_output_dim\n",
    "        self.feat_size = feat_size\n",
    "        self.mfb_out = 1000\n",
    "        self.mfb_factor = 5\n",
    "        self.channel_size = global_avg_pool_size\n",
    "        self.ques_glimse_num = 2\n",
    "        self.img_glimse_num = 2\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size=embedding_size, hidden_size=LSTM_units, \n",
    "                            num_layers=LSTM_layers, batch_first=False)\n",
    "        self.Dropout = nn.Dropout(p=dropout, )\n",
    "        self.Softmax = nn.Softmax()\n",
    "        \n",
    "        self.fc1_q_proj = nn.Linear(LSTM_units*self.ques_glimse_num, self.mfb_output_dim)\n",
    "        self.Conv_i_proj = nn.Conv2d(self.feat_size, self.mfb_output_dim, 1)\n",
    "\n",
    "        self.Dropout_L = nn.Dropout(p=0.2)\n",
    "        self.Dropout_M = nn.Dropout(p=0.2)\n",
    "        self.conv1_q_attn = nn.Conv2d(LSTM_units, 512, 1)\n",
    "        self.conv2_q_attn = nn.Conv2d(512, self.ques_glimse_num, 1)\n",
    "        self.conv1_i_attn = nn.Conv2d(1000, 512, 1)\n",
    "        self.conv2_i_attn = nn.Conv2d(512, self.img_glimse_num, 1)\n",
    "        \n",
    "        self.q_attn_maps = None\n",
    "        self.i_attn_maps = None\n",
    "        \n",
    "    def forward(self, ques_embed, img_feat):\n",
    "        \n",
    "        reshaped_img_feat = img_feat.permute(0, 3, 1, 2).contiguous()         # N x w x w x C -> N x C x w x w\n",
    "        reshaped_img_feat = reshaped_img_feat.reshape(reshaped_img_feat.shape[0], reshaped_img_feat.shape[1], \n",
    "                                              self.channel_size*self.channel_size)      # N x C x w*w\n",
    "        \n",
    "        # ques_embed                                         N x T x embedding_size\n",
    "        reshaped_ques_feat = ques_embed.permute(1, 0, 2).contiguous()        #T x N x embedding_size\n",
    "        lstm_out, (hn, cn) = self.LSTM(reshaped_ques_feat)\n",
    "        lstm1_droped = self.Dropout_L(lstm_out)\n",
    "        lstm1_resh = lstm1_droped.permute(1, 2, 0).contiguous()                   # N x 1024 x T\n",
    "        lstm1_resh2 = torch.unsqueeze(lstm1_resh, 3)                # N x 1024 x T x 1\n",
    "        \n",
    "        '''\n",
    "        Question Attention\n",
    "        '''        \n",
    "        q_attn_conv1 = self.conv1_q_attn(lstm1_resh2)                   # N x 512 x T x 1\n",
    "        q_attn_relu = F.relu(qatt_conv1)\n",
    "        q_attn_conv2 = self.conv2_q_attn(q_attn_relu)                     # N x 2 x T x 1\n",
    "        q_attn_conv2 = q_attn_conv2.reshape(q_attn_conv2.shape[0]*self.ques_glimse_num,-1)\n",
    "        q_attn_softmax = self.Softmax(q_attn_conv2)\n",
    "        q_attn_softmax = q_attn_softmax.view(qatt_conv1.shape[0], self.ques_glimse_num, -1, 1)\n",
    "        self.q_attn_maps = q_attn_softmax\n",
    "        q_attn_feat_list = []\n",
    "        for i in range(self.ques_glimse_num):\n",
    "            t_qatt_mask = q_attn_softmax.narrow(1, i, 1)              # N x 1 x T x 1\n",
    "            t_qatt_mask = t_qatt_mask * lstm1_resh2                 # N x 1024 x T x 1\n",
    "            t_qatt_mask = torch.sum(t_qatt_mask, 2, keepdim=True)   # N x 1024 x 1 x 1\n",
    "            q_attn_feat_list.append(t_qatt_mask)\n",
    "        qatt_feature_concat = torch.cat(q_attn_feat_list, 1)       # N x 2048 x 1 x 1\n",
    "        \n",
    "        '''\n",
    "        Image Attention with MFB\n",
    "        '''\n",
    "        q_feat_resh = torch.squeeze(qatt_feature_concat)                                # N x 2048\n",
    "        i_feat_resh = torch.unsqueeze(reshaped_img_feat, 3)                                   # N x 2048 x w*w x 1\n",
    "        i_attn_q_proj = self.fc1_q_proj(q_feat_resh)                                  # N x 5000\n",
    "        i_attn_q_reshape = i_attn_q_proj.view(i_attn_q_proj.shape[0], self.mfb_output_dim, 1, 1)      # N x 5000 x 1 x 1\n",
    "        i_attn_i_conv = self.Conv_i_proj(i_feat_resh)                                     # N x 5000 x w*w x 1\n",
    "        i_attn_q_conv = i_attn_q_reshape * i_attn_i_conv\n",
    "        i_attn_q_conv_dropped = self.Dropout_M(i_attn_q_conv)                                # N x 5000 x w*w x 1\n",
    "        i_attn_q_conv_permute1 = i_attn_q_conv_dropped.permute(0,2,1,3).contiguous()                              # N x w*w x 5000 x 1\n",
    "        i_attn_q_conv_reshape = i_attn_q_conv_permute1.view(i_attn_q_conv_permute1.shape[0], self.channel_size*self.channel_size, \n",
    "                                             self.mfb_out, self.mfb_factor)\n",
    "        i_attn_q_conv_sums = torch.sum(i_attn_q_conv_reshape, 3, keepdim=True)                      # N x w*w x 1000 x 1 \n",
    "        i_attn_q_conv_permute2 = i_attn_q_conv_sums.permute(0,2,1,3).contiguous()                            # N x 1000 x w*w x 1\n",
    "        i_attn_q_conv_sqrt = torch.sqrt(F.relu(i_attn_q_conv_permute2)) - torch.sqrt(F.relu(-i_attn_q_conv_permute2))\n",
    "        i_attn_q_conv_sqrt = torch.squeeze(i_attn_q_conv_sqrt)\n",
    "        i_attn_q_conv_sqrt = i_attn_q_conv_sqrt.reshape(i_attn_q_conv_sqrt.shape[0], -1)                           # N x 1000*w*w\n",
    "        i_attn_q_conv_l2 = F.normalize(i_attn_q_conv_sqrt)\n",
    "        i_attn_q_conv_l2 = i_attn_q_conv_l2.view(i_attn_q_conv_l2.shape[0], self.mfb_out, self.channel_size*self.channel_size, 1)  # N x 1000 x w*w x 1\n",
    "        \n",
    "        ## 2 conv layers 1000 -> 512 -> 2\n",
    "        i_attn_conv1 = self.conv1_i_attn(i_attn_q_conv_l2)                    # N x 512 x w*w x 1\n",
    "        i_attn_relu = F.relu(i_attn_conv1)\n",
    "        i_attn_conv2 = self.conv2_i_attn(i_attn_relu)                     # N x 2 x w*w x 1\n",
    "        i_attn_conv2 = i_attn_conv2.view(i_attn_conv2.shape[0]*self.img_glimse_num, -1)\n",
    "        i_attn_softmax = self.Softmax(i_attn_conv2)\n",
    "        i_attn_softmax = i_attn_softmax.view(i_attn_conv1.shape[0], self.img_glimse_num, -1, 1)\n",
    "        self.i_attn_maps = i_attn_softmax.view(i_attn_conv1.shape[0], self.img_glimse_num, self.channel_size, self.channel_size)\n",
    "        iatt_feature_list = []\n",
    "        for i in range(self.img_glimse_num):\n",
    "            t_iatt_mask = i_attn_softmax.narrow(1, i, 1)              # N x 1 x w*w x 1\n",
    "            t_iatt_mask = t_iatt_mask * i_feat_resh                 # N x 2048 x w*w x 1\n",
    "            iatt_feature_list.append(t_iatt_mask)\n",
    "        iatt_feature_concat = torch.mean(torch.stack(iatt_feature_list), dim=0)       # N x 2048 x w*w x 1\n",
    "        iatt_feature_resh = iatt_feature_concat.view(i_attn_q_conv_permute1.shape[0], self.channel_size, \n",
    "                                                        self.channel_size, 2048)           # N x w x w x 2048\n",
    "        \n",
    "        return iatt_feature_resh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cac1675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        encoder_dim: feature size of encoded images\n",
    "        decoder_dim: size of decoder's RNN\n",
    "        attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0fbe4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Loads embedding layer with pre-trained embeddings.\n",
    "\n",
    "        embeddings: pre-trained embeddings\n",
    "        \"\"\"\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "\n",
    "        encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "\n",
    "        return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
    "        encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        \n",
    "        return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths\n",
    "        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a074dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2c9bf3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(scores, targets, k):\n",
    "    \"\"\"\n",
    "    Computes top-k accuracy, from predicted and true labels.\n",
    "    scores: scores from the model\n",
    "    targets: true labels\n",
    "    k: k in top-k accuracy\n",
    "    \n",
    "    return: top-k accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = scores.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8f28bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(768, 1024, 2, 2048, 32, 1)\n",
    "decoder = DecoderWithAttention(1024, 1024, 1024, len(word_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6576e3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  0\n",
      "batch no.: 63.69426751592356  loss: 17.62575340270996\n",
      "Epoch -  1\n",
      "batch no.: 63.69426751592356  loss: 10.851142883300781\n",
      "Epoch -  2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[234], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Keep track of metrics\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m top5 \u001b[38;5;241m=\u001b[39m accuracy(scores, targets, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     70\u001b[0m losses\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28msum\u001b[39m(decode_lengths))\n\u001b[0;32m     71\u001b[0m top5accs\u001b[38;5;241m.\u001b[39mupdate(top5, \u001b[38;5;28msum\u001b[39m(decode_lengths))\n",
      "Cell \u001b[1;32mIn[232], line 14\u001b[0m, in \u001b[0;36maccuracy\u001b[1;34m(scores, targets, k)\u001b[0m\n\u001b[0;32m     12\u001b[0m correct \u001b[38;5;241m=\u001b[39m ind\u001b[38;5;241m.\u001b[39meq(targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(ind))\n\u001b[0;32m     13\u001b[0m correct_total \u001b[38;5;241m=\u001b[39m correct\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()  \u001b[38;5;66;03m# 0D tensor\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m correct_total\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m100.0\u001b[39m \u001b[38;5;241m/\u001b[39m batch_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr=0.001\n",
    "n_epochs = 100\n",
    "print_every = 100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()   \n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=encoder_lr)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=decoder_lr)\n",
    "\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "losses = AverageMeter()\n",
    "top5accs = AverageMeter()\n",
    "\n",
    "max_training_acc = 0\n",
    "max_val_acc = 0\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    print('Epoch - ', e)\n",
    "    running_acc = 0\n",
    "    running_loss = 0\n",
    "    counter = 0\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for img, ques, ans, ans_len in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        img = img.cuda()\n",
    "        ques = ques.float().cuda()\n",
    "        ans = ans.cuda().long()\n",
    "        ans_len = ans_len.cuda().long()\n",
    "\n",
    "        encoder_output = encoder( ques, img)\n",
    "\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(encoder_output, ans, ans_len)\n",
    "\n",
    "        targets = caps_sorted[:, 1:]\n",
    "        \n",
    "        scores = (pack_padded_sequence(scores, decode_lengths, batch_first=True)).data\n",
    "        targets = (pack_padded_sequence(targets, decode_lengths, batch_first=True)).data\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += attn_reg * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        # Back prop.\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # Keep track of metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if counter % print_every == 0:\n",
    "            print('batch no.:', (counter*100)/len(train_loader), \n",
    "                  ' loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9dc6f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "val_running_loss = 0\n",
    "val_running_acc = 0\n",
    "for img, ques, ans, ans_len in val_loader:\n",
    "    counter += 1\n",
    "\n",
    "    img = img.cuda()\n",
    "    ques = ques.float().cuda()\n",
    "    ans = ans.cuda().long()\n",
    "    ans_len = ans_len.cuda().long()\n",
    "\n",
    "    encoder_output = encoder(ques, img)\n",
    "\n",
    "    scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(encoder_output, ans, ans_len)\n",
    "\n",
    "    targets = caps_sorted[:, 1:]\n",
    "\n",
    "    scores = (pack_padded_sequence(scores, decode_lengths, batch_first=True)).data\n",
    "    targets = (pack_padded_sequence(targets, decode_lengths, batch_first=True)).data\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(scores, targets)\n",
    "\n",
    "    loss += attn_reg * ((1. - alphas.sum(dim=1)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generation_output(encoder,decoder,img,ques,max_answer_length):\n",
    "    img = img.cuda()\n",
    "    ques = ques.float().cuda()\n",
    "    encoder_output = encoder(ques, img)\n",
    "\n",
    "    start_token=word_to_int['<start>']\n",
    "    pad_token=word_to_int['<pad>']\n",
    "    answer_begining=torch.tensor(np.array([pad_token,start_token]))\n",
    "    decoder_input = torch.tile(answer_begining, (img.shape[0], 1)).cuda()\n",
    "    decoder_input_intial=decoder_input\n",
    "    max_length = max_answer_length  # Setting desired maximum length\n",
    "\n",
    "    # Decode sequentially\n",
    "    for _ in range(max_length):\n",
    "\n",
    "        decoder_length=torch.tensor([decoder_input.shape[1],decoder_input.shape[1]])\n",
    "        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(encoder_output, decoder_input, decoder_length.cuda().long())  # [1] is a length of 1\n",
    "        \n",
    "        predicted_index = torch.argmax(predictions, dim=2)\n",
    "        \n",
    "        decoder_input = torch.cat([decoder_input_intial, predicted_index], dim=1)\n",
    "     \n",
    "    generated_answer=[]\n",
    "    for answer in predicted_index:\n",
    "            a=''\n",
    "            for i in answer:\n",
    "                if int_to_word[i.item()]!='<end>':\n",
    "                    a+=int_to_word[i.item()]+\" \"\n",
    "            generated_answer.append(a)\n",
    "\n",
    "\n",
    "    return generated_answer #answer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f960fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader1 = DataLoader(val_data_tensor, batch_size = 32, shuffle=True)\n",
    "for img, ques, ans, ans_len in val_loader1:\n",
    "  generated_answer=answer_generation_output(encoder,decoder,img,ques,max_answer_length=18)\n",
    "  ground_truth=[]\n",
    "  for answer in ans:\n",
    "    a=''\n",
    "    for i in answer:\n",
    "        if int_to_word[i.item()]!='<end>' and int_to_word[i.item()]!='<pad>' and int_to_word[i.item()]!='<start>':\n",
    "            a+=int_to_word[i.item()]+\" \"\n",
    "    ground_truth.append(a)\n",
    "  break\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "  print(\"Generated\",generated_answer[i])\n",
    "  print(\"True\", ground_truth[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
